{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b97a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7569a4c23174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0mDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7569a4c23174>\u001b[0m in \u001b[0;36mbuildAgent\u001b[1;34m(model, actions)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     dqn = DQNAgent(model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10,\n\u001b[0m\u001b[0;32m     88\u001b[0m                    target_model_update=1e-2)\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Validate (important) input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model \"{}\" has more than one output. DQN expects a model that has a single output.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\keras_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     raise TypeError('Keras symbolic inputs/outputs do not '\n\u001b[0m\u001b[0;32m    222\u001b[0m                     \u001b[1;34m'implement `__len__`. You may be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                     \u001b[1;34m'trying to pass Keras symbolic inputs/outputs '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from keras.layers import Dense, Flatten,Lambda\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import gym\n",
    "\n",
    "\n",
    "class Env(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([39, 27])\n",
    "        self.screen = pygame.display.set_mode((800, 600))\n",
    "        self.PlayerX = 0\n",
    "        self.PlayerY = 0\n",
    "        self.FoodX = 0\n",
    "        self.FoodY = 0\n",
    "        self.state = [self.FoodX - self.PlayerX + 19, self.FoodY - self.PlayerY + 14]\n",
    "        self.timeLimit = 1000\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        pygame.draw.rect(self.screen, (255, 255, 255), pygame.Rect(self.PlayerX * 40, self.PlayerY * 40, 40, 40))\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(self.FoodX * 40, self.FoodY * 40, 40, 40))\n",
    "        pygame.display.update()\n",
    "\n",
    "    def reset(self):\n",
    "        self.FoodX = random.randint(1, 19)\n",
    "        self.FoodY = random.randint(1, 14)\n",
    "        self.PlayerX = 0\n",
    "        self.PlayerY = 0\n",
    "        self.timeLimit = 1000\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.timeLimit -= 1\n",
    "        reward = -1\n",
    "\n",
    "        if action == 0 and self.PlayerY > 0:\n",
    "            self.PlayerY -= 1\n",
    "        if action == 1 and self.PlayerX > 0:\n",
    "            self.PlayerX -= 1\n",
    "        if action == 2 and self.PlayerY < 14:\n",
    "            self.PlayerY += 1\n",
    "        if action == 3 and self.PlayerX < 19:\n",
    "            self.PlayerX += 1\n",
    "\n",
    "        if self.PlayerX == self.FoodX and self.PlayerY == self.FoodY:\n",
    "            reward += 30\n",
    "            self.FoodX = random.randint(1, 19)\n",
    "            self.FoodY = random.randint(1, 14)\n",
    "\n",
    "        if self.timeLimit <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.state = [self.FoodX - self.PlayerX, self.FoodY - self.PlayerY]\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "env = Env()\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(2, activation='relu', input_shape=(1, states[0])))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Lambda(tf.math.argmax, output_shape=1 ))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def buildAgent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10,\n",
    "                   target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "\n",
    "model = build_model(states, actions)\n",
    "DQN = buildAgent(model, actions)\n",
    "\n",
    "DQN.compile(tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "DQN.fit(env, nb_steps=50000, visualize=False, verbose=1)\n",
    "scores = DQN.test(env, nb_episodes=100, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "pygame.quit()\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0b0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: keras-rl2\n",
      "Version: 1.0.5\n",
      "Summary: Deep Reinforcement Learning for Tensorflow 2 Keras\n",
      "Home-page: https://github.com/wau/keras-rl2\n",
      "Author: Taylor McNally\n",
      "Author-email: None\n",
      "License: MIT\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: tensorflow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4ec56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
